{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AP-v9-5-Model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from typing import List\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset and write to pandas.Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/full_data_lemm.csv')\n",
    "df = df.drop(df.columns[[0]], axis = 1)\n",
    "df.iloc[:,0]-=1\n",
    "data = df.dropna()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 31887 #кол-во уникальных слов в датасете \n",
    "cv = CountVectorizer(max_features=max_words)\n",
    "sparse_matrix = cv.fit_transform(data['text']).toarray()\n",
    "feature_names = cv.get_feature_names_out()\n",
    "words = feature_names.tolist()\n",
    "print(feature_names)\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the dataset in the ratio train, test, val: 80,10,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(sparse_matrix, np.array(data['num']),test_size = 0.1, shuffle = True)\n",
    "print(len(x_train))\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1/0.9, shuffle = True)\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting pf.array to tensor and creating TensorDataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_features = torch.Tensor(x_train)\n",
    "train_targets = torch.Tensor(y_train)\n",
    "val_features = torch.Tensor(x_val)\n",
    "val_targets = torch.Tensor(y_val)\n",
    "test_features = torch.Tensor(x_test)\n",
    "test_targets = torch.Tensor(y_test)\n",
    "\n",
    "train = TensorDataset(train_features, train_targets)\n",
    "test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "test_loader_one = DataLoader(test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a LogisticRegression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(31887, 1000)\n",
    "        self.linear2 = nn.Linear(1000, 100)\n",
    "        self.linear3 = nn.Linear(100, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters() , lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train()\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "  loss_values = []\n",
    "  for x_batch, y_batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_batch)\n",
    "    loss = criterion(y_pred, y_batch.long())\n",
    "    loss_values.append(loss.item())\n",
    "    pred = torch.max(y_pred, 1)[1].eq(y_batch).sum()\n",
    "    acc = pred * 100.0 / len(x_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  losses.append(np.array(loss_values).mean())\n",
    "  print('Epoch: {}, Loss: {}, Accuracy: {}%'.format(epoch+1, losses[-1], acc.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Value test vs Epochs plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title('Loss Value test vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Сhecking the model on a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(val_features)\n",
    "    loss = criterion(y_pred, val_targets.long())\n",
    "    pred = torch.max(y_pred, 1)[1].eq(val_targets).sum()\n",
    "    print (\"Accuracy : {}%\".format(100*pred/len(test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model-b1000-lr0001-e10.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for creating your own vector from random feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_update(text: str) -> List[str]:\n",
    "    \"\"\"Function remove from text punctuation marks and split it\n",
    "\n",
    "    Args:\n",
    "        text (str): text for update\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List with words from text\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = text.split()\n",
    "    return ' '.join(text) \n",
    "\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    \"\"\"Function gets text, lemmatize them and removes stopwords\n",
    "\n",
    "    Args:\n",
    "        text (str): text for preprocess \n",
    "\n",
    "    Returns:\n",
    "        List[str]: preprocessed text\n",
    "    \"\"\"\n",
    "    mystem = Mystem()\n",
    "    russian_stopwords = stopwords.words(\"russian\")\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords]\n",
    "    text = \" \".join(tokens)\n",
    "    return text.split()\n",
    "\n",
    "def my_vect(text:str):\n",
    "  tensor = 31887*[0]\n",
    "  text = preprocess_text(text_update(text))\n",
    "  for word in set(text): \n",
    "    if word in words:\n",
    "      tensor[words.index(word)] = text.count(word)\n",
    "  return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for checking the model against random feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text: str):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    y_pred = model(torch.Tensor(my_vect(text)))\n",
    "    return(y_pred)\n",
    "\n",
    "print(predict('Время от времени пользуемся услугами этого интернет магазина, особенно было актуально в период, лакдаун когда все магазины были закрыты. А одежда и обувь была нужна. Как раз на помощь пришёл магазин Wildberries. Но и до всяких корона вирусов мы ей пользовались.'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
